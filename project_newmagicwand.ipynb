{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ucoM13O9Iw"
      },
      "source": [
        "### Charlie BROSSE, Adrien HUET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgxB82d-W9u8"
      },
      "source": [
        "### Nouvelle Baguette Magique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZiA71x_XwN3",
        "outputId": "f9238a79-bf24-4339-b7c7-bc1f3369c446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /home/adhuet/.local/lib/python3.8/site-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /home/adhuet/.local/lib/python3.8/site-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKwwa7A_fvgf"
      },
      "source": [
        "# Statistics and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JiGYGmwQXmtC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import IntegerType"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiqJKKo7gC1O"
      },
      "source": [
        "Spark setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ctndhlR8gCVY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/06/21 22:10:27 WARN Utils: Your hostname, forss resolves to a loopback address: 127.0.1.1; using 172.18.66.219 instead (on interface eth0)\n",
            "22/06/21 22:10:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "22/06/21 22:10:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/06/21 22:10:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        }
      ],
      "source": [
        "spark = (SparkSession.builder.appName('nouvelle_baguette_magique').getOrCreate())\n",
        "spark.sparkContext.setLogLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JU_b0A3f1d3"
      },
      "source": [
        "*Load* the differents csv: we store them in a dictionary where keys are the companies names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xDM8yOtef9jK"
      },
      "outputs": [],
      "source": [
        "csv_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG0HmXBOf02l",
        "outputId": "2bff3fc3-23cf-45d8-88c8-bab625ff5ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AMAZON\n",
            "APPLE\n",
            "FACEBOOK\n",
            "GOOGLE\n",
            "MICROSOFT\n",
            "TESLA\n",
            "ZOOM\n"
          ]
        }
      ],
      "source": [
        "for file in os.listdir('stocks_data'):\n",
        "  if not file.endswith('.csv'):\n",
        "    continue\n",
        "  name = file[:file.rfind('.csv')]\n",
        "  print(name)\n",
        "  df = spark.read.csv('stocks_data/' + file, header=True)\n",
        "  csv_dict[name] = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuUAVbkvhMrf",
        "outputId": "4aba29d1-7178-4a77-f91d-a375d405c734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['AMAZON', 'APPLE', 'FACEBOOK', 'GOOGLE', 'MICROSOFT', 'TESLA', 'ZOOM'])\n"
          ]
        }
      ],
      "source": [
        "companies = csv_dict.keys()\n",
        "print(companies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXlGmJ6dYGHS"
      },
      "source": [
        "Show the first and last 40 rows of each stock price:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV2U3ik0NEbO",
        "outputId": "f80ffa2f-352b-40f1-9131-b55547801522"
      },
      "outputs": [],
      "source": [
        "def first_and_last_40(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Prints the first 40 and the last 40 rows of the stock data of the given DataFrame\n",
        "  \"\"\"\n",
        "  print('HEAD:')\n",
        "  print(df.head(40))\n",
        "  print('TAIL:')\n",
        "  print(df.tail(40))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWFyy2cWc8EY"
      },
      "source": [
        "Get the number of observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RPGX1LZc-Tv",
        "outputId": "97cdd705-218e-4c16-e3d8-54bed2b53def"
      },
      "outputs": [],
      "source": [
        "def get_nb_obs():\n",
        "  \"\"\"\n",
        "  Return the total number of observations in the dataframes of csv_dict (csv_dict must be defined earlier)\n",
        "  \"\"\"\n",
        "  count = 0\n",
        "  for company in companies:\n",
        "    count += csv_dict[company].count()\n",
        "  return count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBjosG-0c-wP"
      },
      "source": [
        "Deduce programmatically what is the period you have between the data points : for\n",
        "example, if you have data point with the following date [01/01, 02/01, …..], you\n",
        "shoud have a function that will analyse the difference between the dates\n",
        "automatically and deduce it is a day period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xtevdeuqdGBw"
      },
      "outputs": [],
      "source": [
        "def seconds_to_period(seconds: float):\n",
        "  \"\"\"\n",
        "  Get the order of time (hours, days, weeks, months, years) of the given number of seconds.\n",
        "  For example, seconds_to_period(3600) = 'hours'\n",
        "  \"\"\"\n",
        "  conversions = {'minutes': 60,\n",
        "                'hours': 3600,\n",
        "                'days': 3600*24,\n",
        "                'weeks': 3600*24*7,\n",
        "                'months': 3600*24*7*3,\n",
        "                'year': 3600*24*7*52}\n",
        "\n",
        "  prev = 'seconds'\n",
        "  for order in conversions:\n",
        "      if seconds / conversions[order] < 1:\n",
        "        return prev\n",
        "      prev = order\n",
        "  return 'year'\n",
        "        \n",
        "def get_order(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Return the the sample period (str) of the given dataframe\n",
        "  \"\"\"\n",
        "  # Only get the dates\n",
        "  dates = df.select(to_date(df.Date).alias('ts'))\n",
        "\n",
        "  # Duplicating the ts column with a shift of 1, so that each row contains the current timestamp and the previous one\n",
        "  my_window = Window.partitionBy().orderBy(\"ts\")\n",
        "  dates = dates.withColumn(\"prev_ts\", lag(dates.ts).over(my_window))\n",
        "\n",
        "  # Get the difference in seconds between ts and the newly created 'prev_ts' column\n",
        "  dates = dates.withColumn(\"diff\", unix_timestamp(dates.ts) - unix_timestamp(dates.prev_ts))\n",
        "  # Get the average of the diff\n",
        "  diff_avg = dates.agg({'diff': 'mean'}).collect()[0][0]\n",
        "\n",
        "  # The diff is the average step between two timestamp, this finds its order of time (seconds, minutes, hours, ...)\n",
        "  order = seconds_to_period(diff_avg)\n",
        "  return order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SiXcQPcdGXm"
      },
      "source": [
        "Descriptive statistics for each dataframe and each column (min, max, standard\n",
        "deviation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aKy8FgJJSeQW"
      },
      "outputs": [],
      "source": [
        "def show_descriptive_stats(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Prints on the output descriptive statistics for the given DataFrame\n",
        "  \"\"\"\n",
        "  df.summary().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFcjBHxjdKoE"
      },
      "source": [
        "Number of missing values for each dataframe and column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OfdSwxCqdObL"
      },
      "outputs": [],
      "source": [
        "def show_missing_values(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Returns a DataFrame containing the number of NaNs and null values for each column in the given DataFrame\n",
        "  \"\"\"\n",
        "  return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1nZCzuxtt1Y"
      },
      "source": [
        "\tThere are no missing values nor null values except in company name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npr3TABPdO7T"
      },
      "source": [
        "Correlation between values "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCGFRp_U1zWy"
      },
      "source": [
        "Voici les matrices de corrélations entre chaque colonnes pour chaque entreprises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BUmsdGu7dS4a"
      },
      "outputs": [],
      "source": [
        "def show_correlation_matrix(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Returns the correlation matrix over the columns of the given DataFrame\n",
        "  \"\"\"\n",
        "  corr_mat = np.empty(6, dtype=object)\n",
        "  corr_mat[...] = [[] for _ in range(corr_mat.shape[0])]\n",
        "  copy = df.alias(\"copy\")\n",
        "  cols = copy.columns[1:-1]\n",
        "\n",
        "  for c in cols:\n",
        "    copy = copy.withColumn(c, copy[c].cast(IntegerType()))\n",
        "\n",
        "  for i, col1 in enumerate(cols):\n",
        "    for j, col2 in enumerate(cols):\n",
        "      corr_mat[i].append(copy.stat.corr(col1, col2))\n",
        "  \n",
        "  return corr_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfrPhQgDfHCJ"
      },
      "source": [
        "What is the average of the opening and closing prices for each stock price and for\n",
        "different time periods (week, month, year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v-QXQWIjfID1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "def show_average_open_close_stock_per_timeperiod(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Returns a tuple of DataFrames containing the weekly, monthly and yearly average of the stock prices for open and close in the given DataFrame\n",
        "  \"\"\"\n",
        "  # week\n",
        "  weekly_open_close = df.withColumn(\"week_strt_day\",date_sub(next_day(col(\"Date\"),\"sunday\"),7)).groupBy(\"week_strt_day\").agg(mean(\"Open\").cast(\"int\").alias(\"Open\"),mean(\"Close\").cast(\"int\").alias(\"Close\")).orderBy(\"week_strt_day\")\n",
        "\n",
        "  # month\n",
        "  monthly_open_close = df.withColumn(\"month_strt_day\",substring('Date', 0, 7)).groupBy(\"month_strt_day\").agg(mean(\"Open\").cast(\"int\").alias(\"Open\"),mean(\"Close\").cast(\"int\").alias(\"Close\")).orderBy(\"month_strt_day\")\n",
        "  \n",
        "  # year\n",
        "  yearly_open_close = df.withColumn(\"year_strt_day\",substring('Date', 0, 4)).groupBy(\"year_strt_day\").agg(mean(\"Open\").cast(\"int\").alias(\"Open\"),mean(\"Close\").cast(\"int\").alias(\"Close\")).orderBy(\"year_strt_day\")\n",
        "\n",
        "  return [weekly_open_close, monthly_open_close, yearly_open_close]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeekIJmDfJut"
      },
      "source": [
        "How do the stock prices change day to day and month to month (may be you can\n",
        "create new columns to save those calculations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iOyJFOjzfK6Y"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import lag, col\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "def show_stock_price_evolution_per_timeperiod(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Return a couple of DataFrames containing the stock price evolution per timeperiod.\n",
        "  \"\"\"\n",
        "  # day by day evolution for Close\n",
        "  w1 = Window().partitionBy().orderBy(col(\"Date\"))\n",
        "  df_with_day_evol = df.select(\"*\", lag(\"Close\").over(w1).alias(\"tmp\")).na.drop()\n",
        "  df_with_day_evol = df_with_day_evol.withColumn(\"day_evolution\", df_with_day_evol[\"Close\"] - df_with_day_evol[\"tmp\"])\n",
        "  df_with_day_evol = df_with_day_evol.drop(df_with_day_evol[\"tmp\"])\n",
        "  \n",
        "\n",
        "  # month by month evolution for Close\n",
        "  w2 = Window().partitionBy().orderBy(col(\"month_strt_day\"))\n",
        "\n",
        "  df_with_monthly_evol = df.withColumn(\"month_strt_day\",substring('Date', 0, 7)).groupBy(\"month_strt_day\").agg(mean(\"Close\").cast(\"int\").alias(\"Close\")).orderBy(\"month_strt_day\")\n",
        "  df_with_monthly_evol = df_with_monthly_evol.select(\"*\", lag(\"Close\").over(w2).alias(\"tmp\")).na.drop()\n",
        "  df_with_monthly_evol = df_with_monthly_evol.withColumn(\"month_evolution\", df_with_monthly_evol[\"Close\"] - df_with_monthly_evol[\"tmp\"])\n",
        "  df_with_monthly_evol = df_with_monthly_evol.drop(df_with_monthly_evol[\"tmp\"])\n",
        "\n",
        "  return (df_with_day_evol, df_with_monthly_evol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T_NPul4fNpL"
      },
      "source": [
        "Based on the opening and closing price, calculate the daily return of each stock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fGENVr2GfOMF"
      },
      "outputs": [],
      "source": [
        "def show_daily_return(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Return an updated version of the given DataFrame with an additional column containing the daily evolution of the stock price\n",
        "  \"\"\"\n",
        "  df_with_daily_return = df.withColumn(\"daily_return\", df[\"Close\"] - df[\"Open\"])\n",
        "  return df_with_daily_return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgg4ShLefPxA"
      },
      "source": [
        "What are the stocks with the highest daily return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EBPuHNJpfQZc"
      },
      "outputs": [],
      "source": [
        "def show_max_daily_return(df: DataFrame):\n",
        "  \"\"\"\n",
        "  Return the maximum values of daily_return from the given DataFrame\n",
        "  \"\"\"\n",
        "  return df.groupby().max(\"daily_return\").first().asDict()['max(daily_return)']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdi8SyInfSHd"
      },
      "source": [
        "Calculate the average daily return for different periods (week, month, and year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nTqrkgKufS21"
      },
      "outputs": [],
      "source": [
        "def show_average_daily_return_per_timeperiod(df):\n",
        "  \"\"\"\n",
        "  Returns a tuple of DataFrames containing the weekly, monthly and yearly average of the stock price daily evolution computed from the given DataFrame\n",
        "  \"\"\"\n",
        "  # week\n",
        "  weekly = df.withColumn(\"week_strt_day\",date_sub(next_day(col(\"Date\"),\"sunday\"),7)).groupBy(\"week_strt_day\").agg(mean(\"daily_return\").cast(\"int\").alias(\"average_daily_return\")).orderBy(\"week_strt_day\")\n",
        "\n",
        "  # month\n",
        "  monthly = df.withColumn(\"month_strt_day\",substring('Date', 0, 7)).groupBy(\"month_strt_day\").agg(mean(\"daily_return\").cast(\"int\").alias(\"average_daily_return\")).orderBy(\"month_strt_day\")\n",
        "  \n",
        "  # year\n",
        "  yearly = df.withColumn(\"year_strt_day\",substring('Date', 0, 4)).groupBy(\"year_strt_day\").agg(mean(\"daily_return\").cast(\"int\").alias(\"average_daily_return\")).orderBy(\"year_strt_day\")\n",
        "\n",
        "  return [weekly, monthly, yearly]\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLXkrm9OfZiK"
      },
      "source": [
        "**Moving average** : The moving average is calculated by adding a stock's prices over a certain\n",
        "period and dividing the sum by the total number of periods. For example, if you want to\n",
        "calculate the moving average for the opening price of the stock ABC. You look at the opening\n",
        "price over five periods and calculate the average. For example if the opening price over the\n",
        "past five days were 25.40, 25.90. 26.50, 26.30 and 27.90. Then, the moving average of the\n",
        "opening price of the last day is 26.40. **Code a function** that take as input a dataframe, a\n",
        "column name, the number of points to consider for the moving average (5 in the example)\n",
        "and add a new column to the dataframe with the values of calculated moving average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lXmSSZmofZLG"
      },
      "outputs": [],
      "source": [
        "def moving_average(df: DataFrame, column: str, nb_periods: int):\n",
        "  \"\"\"\n",
        "  Returns a updated version of the given DataFrame containing the rolling average of the specified column over the given number of periods\n",
        "  The new column is given the name 'roll_[column]_[nb_periods]'\n",
        "  \"\"\"\n",
        "  conversions = {'minutes': 60,\n",
        "                'hours': 3600,\n",
        "                'days': 3600*24,\n",
        "                'weeks': 3600*24*7,\n",
        "                'months': 3600*24*7*3,\n",
        "                'year': 3600*24*7*52}\n",
        "  order = get_order(df)\n",
        "\n",
        "  df = df.withColumn('timestampGMT', df.Date.cast('timestamp'))\n",
        "\n",
        "  # create window by casting timestamp to long (number of seconds)\n",
        "  w = (Window.orderBy(col(\"timestampGMT\").cast('long')).rangeBetween(-(nb_periods * conversions[order]), 0))\n",
        "\n",
        "  df = df.withColumn('roll_' + column + '_' + str(nb_periods), avg(column).over(w))\n",
        "  return df.drop('timestampGMT')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAfEFCrifiVc"
      },
      "source": [
        "**Correlation** : Is there any correlation between the different stocks you have ? **Code a\n",
        "function** that takes as input the values of two stocks (you should decide what is the data\n",
        "type that will handle the values) and calculate the correlation between them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gx0MyCveXKQ4"
      },
      "outputs": [],
      "source": [
        "def corr_between(df1: DataFrame, df2: DataFrame, column: str):\n",
        "  \"\"\"\n",
        "  Returns the correlation coeficient between the two given DataFrames over the given column\n",
        "  \"\"\"\n",
        "  df = df1.select(col(column).cast('float')).withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
        "  df_other = df2.select(col(column).cast('float')).withColumnRenamed(column, column + '_other').withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
        "  joined = df.join(df_other, on=['row_index']).drop('row_index')\n",
        "  return joined.stat.corr(column, column + '_other')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YRXSwvW9BLY"
      },
      "source": [
        "When investing in stocks, the return rate is very important. **Code a function** that calculates\n",
        "the return rate of the stock in different periods (week, month and year) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39KWCvwlXN_L",
        "outputId": "294dbee0-33b2-46a8-a1c4-e75b2f7c9930"
      },
      "outputs": [],
      "source": [
        "def compute_return_rate(df: DataFrame, period: str = 'year'):\n",
        "  \"\"\"\n",
        "  Returns a DataFrame containing the returns rate with dates spanning in the fashion of the given period (defaults to 'year')\n",
        "  \"\"\"\n",
        "  # Extract year, month and week from df\n",
        "  split = df.withColumn(\n",
        "      'year', year(df.Date)).withColumn(\n",
        "      'month', month(df.Date)).withColumn(\n",
        "      'week', weekofyear(df.Date))\n",
        "\n",
        "  date_window = Window.partitionBy().orderBy('Date')\n",
        "  with_prev = split.withColumn('prev_value', lag(split['Adj Close']).over(date_window))\n",
        "  with_diff = with_prev.withColumn('diff', with_prev['Adj Close'] - with_prev['prev_value'])\n",
        "  pct_change = with_diff.withColumn('pct_change', with_diff['diff'] / with_diff['prev_value'] * 100)\n",
        "\n",
        "  if period == 'year':\n",
        "    return pct_change.groupBy('year').agg(sum('pct_change').alias('return_rate'))\n",
        "  elif period == 'month':\n",
        "    return pct_change.groupBy('year', 'month').agg(sum('pct_change').alias('return_rate'))\n",
        "  elif period == 'week':\n",
        "    return pct_change.groupBy('year', 'week').agg(sum('pct_change').alias('return_rate'))\n",
        "  elif period == 'day':\n",
        "    return pct_change.drop('prev_value', 'diff', 'year', 'month', 'week').withColumnRenamed('pct_change', 'return_rate')\n",
        "  else:\n",
        "    raise ValueError(f'{period} is not a valid period. Should be either \"year\", \"month\", \"week\" or \"day\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMNdU-EM9HTZ"
      },
      "source": [
        "Given a specific month, what is the stock with the best return rate. **Code a function** that\n",
        "takes as input a start date and a period (month, year), calculate the return rate for each\n",
        "stock and return the one with the best return rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kGUOWX9T9JK0"
      },
      "outputs": [],
      "source": [
        "def return_rate_over_period(df: DataFrame, start_date: datetime, end_date: datetime):\n",
        "  \"\"\"\n",
        "  Compute and returns the return rate between two dates for the given DataFrame\n",
        "  \"\"\"\n",
        "  ranged = df.filter(df.Date >= start_date).filter(df.Date <= end_date)\n",
        "  \n",
        "  date_window = Window.partitionBy().orderBy('Date')\n",
        "  with_prev = ranged.withColumn('prev_value', Functions.lag(ranged['Adj Close']).over(date_window))\n",
        "  with_diff = with_prev.withColumn('diff', with_prev['Adj Close'] - with_prev['prev_value'])\n",
        "  pct_change = with_diff.withColumn('pct_change', with_diff['diff'] / with_diff['prev_value'] * 100)\n",
        "  total = pct_change.select('pct_change').groupBy().sum().collect()[0][0]\n",
        "  return total\n",
        "\n",
        "def best_return_rate_on_period(companies_csv_dict: dict, start_date: str, period: str):\n",
        "  \"\"\"\n",
        "  Return the name of the company which has the highest return rate over a period after a given starting date, and the actual best return rate over the period\n",
        "  \"\"\"\n",
        "  start = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "  if period == 'year':\n",
        "    end = start + relativedelta(years=1)\n",
        "  elif period == 'month':\n",
        "    end = start + relativedelta(months=1)\n",
        "  else:\n",
        "    raise ValueError(f'{period} is not a valid period. Should be either \"year\" or \"month\"')\n",
        "\n",
        "  comp, max = None, 0\n",
        "  for company in companies:\n",
        "    data = companies_csv_dict[company]\n",
        "    rr = return_rate_over_period(data, start, end)\n",
        "    if rr is not None and rr >= max:\n",
        "      comp = company\n",
        "      max = rr\n",
        "\n",
        "  return comp, max"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Full Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncomment parts to be ran and tweak parameters if needed (some example are given)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AMAZON\n"
          ]
        }
      ],
      "source": [
        "for company in companies:\n",
        "  print(company)\n",
        "  df = csv_dict[company]\n",
        "\n",
        "  # print(\"Descriptive statistics\")\n",
        "  # show_descriptive_stats(df)\n",
        "  \n",
        "  # print(\"Missing values\")\n",
        "  # show_missing_values(df).show()\n",
        "  \n",
        "  # print(\"Correlation matrix\")\n",
        "  # print(show_correlation_matrix(df))\n",
        "  \n",
        "  # print(\"Average open and close stock weekly, monthly and yearly\")\n",
        "  # avg_daily, avg_monthly, avg_yearly = show_average_open_close_stock_per_timeperiod(df)\n",
        "  # # avg_daily.show(10)\n",
        "  # avg_monthly.show(10)\n",
        "  # avg_yearly.show(10) \n",
        "  \n",
        "  # print(\"Stock price evolution each day and month\")\n",
        "  # daily_evol, monthly_evol = show_stock_price_evolution_per_timeperiod(df)\n",
        "  # daily_evol.show(10)\n",
        "  # monthly_evol.show(10)\n",
        "\n",
        "  # print(\"Daily return\")\n",
        "  # daily_return_df = show_daily_return(df)\n",
        "  # daily_return_df.show()\n",
        "\n",
        "  # print(\"Max daily return\")\n",
        "  # print(show_max_daily_return(daily_return_df))\n",
        "\n",
        "  # print(\"Average daily return weekly, monthly and yearly\")\n",
        "  # avg_dr_week, avg_dr_month, avg_dr_year = show_average_daily_return_per_timeperiod(daily_return_df)\n",
        "  # avg_dr_week.show(10)\n",
        "  # avg_dr_month.show(10)\n",
        "  # avg_dr_year.show(10)\n",
        "\n",
        "  # print('Rolling average (Adj Close, 5 periods)')\n",
        "  # rolling_df = moving_average(df, 'Adj Close', 5)\n",
        "  # rolling_df.show()\n",
        "\n",
        "  # print('Return rate over years')\n",
        "  # yearly_rr = compute_return_rate(df, 'year')\n",
        "  # yearly_rr.show()\n",
        "\n",
        "  # Uncomment below to run for ALL companies, not just AMAZON\n",
        "  break\n",
        "\n",
        "# print('Example correlation between APPLE and FACEBOOK over Adj Close')\n",
        "# df1 = csv_dict['APPLE']\n",
        "# df2 = csv_dict['FACEBOOK']\n",
        "# print(corr_between(df1, df2, 'Adj Close'))\n",
        "\n",
        "# print('Best performing company for a year after 2019-01-01')\n",
        "# print(best_return_rate_on_period(csv_dict, '2019-01-01', 'year'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d97sjzWLiQDt"
      },
      "source": [
        "# Modèle de prédiction de la valeur des actions\n",
        "\n",
        "- Séparation du dataset en train et test sets\n",
        "- Préparer le dataset (transformers)\n",
        "- Construire le modèle (estimators)\n",
        "- Créer la pipeline\n",
        "- Evaluer le modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-uzSbZZsD_"
      },
      "source": [
        "Dans la cellule suivante nous séparons les étapes de la régression en différentes fonctions par soucis de clarité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tVheE-y99K8E"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "\n",
        "def separateTrainTestsets(df):\n",
        "  # Train / Test sets\n",
        "  dftrain, dftest = df.randomSplit([.8, .2], seed=42)\n",
        "  print(f\"There are {dftrain.cache().count()} rows in the training set, and {dftest.cache().count()} in the test set\")\n",
        "\n",
        "  return dftrain, dftest \n",
        "\n",
        "def prepareDataset(dftrain, inputCols):\n",
        "  # Prepare dataset\n",
        "  vecAssembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
        "  vecTrainDf = vecAssembler.transform(dftrain)\n",
        "  \n",
        "  return vecAssembler, vecTrainDf \n",
        "\n",
        "def buildModel(vecTrainDf, labelCol):\n",
        "  # Build model\n",
        "  lr = LinearRegression(featuresCol=\"features\", labelCol=labelCol)\n",
        "  lrModel = lr.fit(vecTrainDf)\n",
        "\n",
        "  return lr, lrModel\n",
        "\n",
        "def buildPipeline(trainDF, stages):\n",
        "  # Build pipeline\n",
        "  pipeline = Pipeline(stages=stages)\n",
        "  pipelineModel = pipeline.fit(trainDF)\n",
        "  \n",
        "  return pipelineModel\n",
        "\n",
        "def predictTestSet(testDF, pipelineModel):\n",
        "  # Test on test set\n",
        "  predDF = pipelineModel.transform(testDF)\n",
        "  \n",
        "  return predDF\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE2DAkexZ5Ku"
      },
      "source": [
        "Dans la cellule suivante nous convertissons les différentes colonnes intéressantes pour notre régression en \"double\" afin qu'elles soient utilisables pour cette dernière."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Y3D0yfghzrV2"
      },
      "outputs": [],
      "source": [
        "def cast_string_to_double(csv_dict):\n",
        "  # Cast Date column to date\n",
        "  pred_csv_dict = csv_dict.copy()\n",
        "  for company in companies:\n",
        "    pred_csv_dict[company] = pred_csv_dict[company].withColumn(\"Date\", unix_timestamp(\"Date\", \"yyyy-MM-dd\"))\n",
        "    pred_csv_dict[company] = pred_csv_dict[company].withColumn(\"Close\",csv_dict[company].Close.cast('double'))\n",
        "    pred_csv_dict[company] = pred_csv_dict[company].withColumn(\"Open\",csv_dict[company].Open.cast('double'))\n",
        "    pred_csv_dict[company] = pred_csv_dict[company].withColumn(\"Adj Close\",csv_dict[company][\"Adj Close\"].cast('double'))\n",
        "  \n",
        "  return pred_csv_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vtbRjUHaGo2"
      },
      "source": [
        "La fonction suivante réunie les différentes fonctions précédentes de régression (hormis celle de conversion) afin d'effectuer la régression.\n",
        "Le résultat est un dictionnaire contenant, pour chaque entreprises, un dataframe similaire au précédent mais contenant en plus une colonne avec les prédictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "faSkJcSjqmPF"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def perform_regression(df):\n",
        "\n",
        "\n",
        "  # Build model and predict\n",
        "\n",
        "  dftrain, dftest = separateTrainTestsets(df)\n",
        "  vecAssembler, vecTrainDf = prepareDataset(dftrain, [\"Date\", \"Open\"])\n",
        "  lrClose, lrModelClose = buildModel(vecTrainDf, \"Adj Close\")\n",
        "  pipelineModelClose = buildPipeline(dftrain, [vecAssembler, lrClose])\n",
        "  predDFClose = predictTestSet(dftest, pipelineModelClose)\n",
        "  \n",
        "  return predDFClose\n",
        "\n",
        "def perform_regression_dict(prep_csv_dict):\n",
        "  pred_dict = {}\n",
        "\n",
        "  for company in companies:\n",
        "    df = prep_csv_dict[company]\n",
        "    pred_dict[company] = perform_regression(df)\n",
        "\n",
        "  return pred_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kJJIFMwaval"
      },
      "source": [
        "Les fonctions suivantes calculent respectivement les metrics et les plots montrant les résultats des régressions faites par la fonction d'avant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fen6uhdwudy8"
      },
      "outputs": [],
      "source": [
        "def compute_rmse(df):\n",
        "\n",
        "  # Metrics\n",
        "\n",
        "  predDFClose = df.withColumn(\"absolute error\", abs(df[\"prediction\"] - df[\"Adj Close\"]))\n",
        "  regressionMeanEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Adj Close\", metricName=\"rmse\")\n",
        "  rmse = regressionMeanEvaluator.evaluate(predDFClose)\n",
        "  max_abse = predDFClose.sort(col(\"absolute error\").desc()).collect()[0][\"absolute error\"]\n",
        "\n",
        "  return rmse, max_abse\n",
        "\n",
        "def compute_rmse_dict(pred_dict):\n",
        "  rmse_dict = {}\n",
        "  max_abse_dict = {}\n",
        "\n",
        "  for company in companies:\n",
        "    df = pred_dict[company]\n",
        "    rmse_dict[company], max_abse_dict[company] = compute_rmse(df)\n",
        "  \n",
        "  return rmse_dict, max_abse_dict\n",
        "\n",
        "def plot_predictions_dict(pred_dict):\n",
        "\n",
        "  # Plot\n",
        "  f, axs = plt.subplots(7, figsize=(12, 20))\n",
        "  plot_index = 0\n",
        "\n",
        "  for company in companies:\n",
        "    predDFClose = pred_dict[company]\n",
        "    dates = predDFClose.select(from_unixtime('Date', 'MM-dd-yyyy')).rdd.map(lambda x : x[0]).collect()\n",
        "\n",
        "    predictions = predDFClose.select('prediction').rdd.map(lambda x : x[0]).collect()\n",
        "    adjCloses = predDFClose.select('Adj Close').rdd.map(lambda x : x[0]).collect()\n",
        "    \n",
        "    \n",
        "    axs[plot_index].plot(dates, adjCloses, markersize=2, label='Real Close value')\n",
        "    axs[plot_index].plot(dates, predictions, markersize=2, label='Prediction')\n",
        "    \n",
        "    axs[plot_index].set_ylabel('Stock value')\n",
        "    axs[plot_index].set_xlabel('timestamp')\n",
        "    axs[plot_index].set_xticks(dates[::50])\n",
        "    axs[plot_index].legend()\n",
        "\n",
        "    plot_index += 1\n",
        "\n",
        "def plot_df(df):\n",
        "  dates = df.select(from_unixtime('Date', 'MM-dd-yyyy')).rdd.map(lambda x : x[0]).collect()\n",
        "\n",
        "  predictions = df.select('prediction').rdd.map(lambda x : x[0]).collect()\n",
        "  adjCloses = df.select('Adj Close').rdd.map(lambda x : x[0]).collect()\n",
        "  \n",
        "  \n",
        "  plt.plot(dates, adjCloses, markersize=2, label='Real Close value')\n",
        "  plt.plot(dates, predictions, markersize=2, label='Prediction')\n",
        "  \n",
        "  plt.set_ylabel('Stock value')\n",
        "  plt.set_xlabel('timestamp')\n",
        "  plt.set_xticks(dates[::50])\n",
        "  plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RramjmOcbB3c"
      },
      "source": [
        "La cellule suivante regroupe et appelle les fonctions de la régression pour notre use case. Il s'agit d'un modèle par entreprise. En effet, il n'y a aucun intérêt ici à créer un modèle commun pour les entreprises car chaque entreprise veut pouvoir prédire ses propres résultats et non pas ceux d'un groupe d'entreprises (les résultats seraient erronés). Cela aurait du sens si les entreprises faisaient partie d'un même groupe financier, auquel cas il aurait été pratique de prédire le cours de l'action du groupe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "oG5SSyDuZaj2",
        "outputId": "dc49f799-f7d1-4f80-bc18-5978150f7b23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 828 rows in the training set, and 159 in the test set\n",
            "There are 828 rows in the training set, and 159 in the test set\n",
            "There are 828 rows in the training set, and 159 in the test set\n",
            "There are 828 rows in the training set, and 159 in the test set\n",
            "There are 828 rows in the training set, and 159 in the test set\n",
            "There are 828 rows in the training set, and 159 in the test set\n",
            "There are 352 rows in the training set, and 59 in the test set\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9ae7f2a790>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD6CAYAAABOIFvoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuFElEQVR4nO3de3wU1d3H8c+PiBIBwQtSbhqwigpELvFWL6DYYlWoWkXoDUSrUhGlgoqtotZWWqy2ttRHHluh1kehVKmXWmtRqq22Jcgd5CJiTfCCVCJCkAR+zx8zGzZhN5lNdrO5fN+v175258zOnN/M7s7ZOXPmHHN3REREAFpkOwAREWk4VCiIiEgFFQoiIlJBhYKIiFRQoSAiIhVUKIiISAUVCiIiUmG/bAdQF4cddpjn5eVlOwwRkUZl0aJFH7l7h0TzaiwUzKwj8COgs7t/2cyOB05191+nOc6U5eXlUVhYmO0wREQaFTN7J9m8KNVHM4EXgM7h9FrghjpHJSIiDU6UQuEwd58D7AFw93Jgd0ajEhGRrIhSKGw3s0MBBzCzU4CSjEYlIiJZEeVC83eBp4GjzOwfQAfgkoxGJdJElJWVUVRUxM6dO7MdijRDrVq1omvXrrRs2TLyMjUWCu7+hpkNBHoCBqxx97LahynSfBQVFdG2bVvy8vIws2yHI82Iu7NlyxaKioro3r175OWitD76VpWk/maGu/821SAbhGVzYP5dUFIE7brC4Nshf3i2o5ImaufOnSoQJCvMjEMPPZTNmzentFyU6qMT4163AgYDbwCNr1BYNgeeGQ9lpcF0ybvBNKhgkIxRgSDZUpvvXpTqo+uqZNIeeCLlnBqC+XftLRBiykqDdBUKIiK16uZiOxC9gqohKSlKLV2kCcjJyaFv37707t2boUOHsnXr1lqtZ+bMmWzatCm9wTVho0ePZu7cudkOI2U1Fgpm9oyZPR0+ngXWAE9FWO43Zvahma2ISzvEzF40s3Xh88FhupnZA2a23syWmVn/umxUUu26ppYu0gTk5uayZMkSVqxYwSGHHML06dNrtR4VClBeXp6VfKLmm474opwp3Av8NHzcA5zp7rdEWG4mcG6VtFuA+e5+NDA/nAb4MnB0+LgKeDDC+lM3+HZomVs5rWVukC7SAMxbXMxpU1+i+y3PcdrUl5i3uDit6z/11FMpLg7WuWTJEk455RTy8/O56KKL+Pjjj5Omz507l8LCQr7+9a/Tt29fSksrV8MOGjSICRMmUFBQwHHHHcfChQu5+OKLOfroo/n+979f8b7f/e53nHTSSfTt25err76a3buD+2DHjh1LQUEBvXr1YsqUKRXvz8vLY8qUKfTv358+ffrw5ptv7rNNK1eurFhnfn4+69atA+CHP/whxxxzDKeffjojR47k3nvvrYg11j3ORx99RKz/tI0bN3LGGWfQv39/+vfvz2uvvQbAggULOOOMMxg2bBjHH388u3fvZtKkSZx44onk5+fz0EMPAUFrn3HjxtGzZ0/OOeccPvzww4SfwVtvvcW5557LgAEDOOOMMyq2afTo0VxzzTWcfPLJ3HTTTftMJ/u8Bg0axA033EBBQQE///nPI30PquXuGXsAecCKuOk1QKfwdSeC5q0ADwEjE72vuseAAQM8ZUtnu9/Xy31Ku+B56ezU1yES0apVqyK/96k3ivzY7z/vR978bMXj2O8/70+9UVSnGFq3bu3u7uXl5X7JJZf4888/7+7uffr08QULFri7+2233ebXX399tekDBw70hQsXJsxj4MCBftNNN7m7+89+9jPv1KmTb9q0yXfu3OldunTxjz76yFetWuUXXHCB79q1y93dx44d67NmzXJ39y1btlTEOHDgQF+6dKm7ux955JH+wAMPuLv79OnT/Yorrtgn73Hjxvnvfvc7d3f/7LPPfMeOHV5YWOi9e/f27du3e0lJiR911FE+bdq0fbZj8+bNfuSRR7q7+/bt2720tNTd3deuXeux48vLL7/sBx54oG/YsMHd3R966CH/wQ9+4O7uO3fu9AEDBviGDRv8D3/4g59zzjleXl7uxcXF3q5dO//973+/T7xnn322r1271t3d//nPf/pZZ53l7u6jRo3y888/38vLyxNOV/e5jB07NuHn4p74OwgUepLjatILzWa2jfAu5qqzgrLED6pFGdTR3d8LX78PdAxfdwHejXtfUZj2HumWP1wXlaVBmvbCGkrLKvcgU1q2m2kvrOHCfl1qvd7S0lL69u1LcXExxx13HF/84hcpKSlh69atDBw4EIBRo0Zx6aWXJk2PYtiwYQD06dOHXr160alTJwB69OjBu+++y9///ncWLVrEiSeeWBHX4YcfDsCcOXOYMWMG5eXlvPfee6xatYr8/HwALr74YgAGDBjAk08+uU++p556Kj/84Q8pKiqqODt59dVXueiiizjwwAMrxVadsrIyxo0bx5IlS8jJyWHt2rUV80466aSKtv5/+ctfWLZsWcX1gpKSEtatW8crr7zCyJEjycnJoXPnzpx99tn75PHpp5/y2muvVdqnn332WcXrSy+9lJycnH2ma/pcLrvsshq3L6qkhYK7t01bLonX72aWqNCplpldRVDFxBFHHJH2uESyZdPW0pTSo4pdU9ixYwdDhgxh+vTpjBo1qk7rTOSAAw4AoEWLFhWvY9Pl5eW4O6NGjeKee+6ptNzbb7/Nvffey8KFCzn44IMZPXp0pTvAY+vKyclJWGf+ta99jZNPPpnnnnuO8847r6I6J5n99tuPPXv2AFTK5/7776djx44sXbqUPXv20KpVq4p5rVu3rnjt7vziF79gyJAhldb7pz/9qdp8Afbs2UP79u1ZsmRJwvnx+SSaTibq+6KI3PrIzA43syNij1rm94GZdQrX1wmIVboVA93i3tc1TNuHu89w9wJ3L+jQIWF34CKNUuf2uSmlp+rAAw/kgQce4Kc//SmtW7fm4IMP5tVXXwXg0UcfZeDAgbRr1y5hOkDbtm3Ztm1brfMfPHgwc+fOrahr/+9//8s777zDJ598QuvWrWnXrh0ffPABzz//fErr3bBhAz169GD8+PF85StfYdmyZZx55pnMmzeP0tJStm3bxjPPPFPx/ry8PBYtWgRQqXVQSUkJnTp1okWLFjz66KMV1zuqGjJkCA8++CBlZUHHDmvXrmX79u2ceeaZzJ49m927d/Pee+/x8ssv77PsQQcdRPfu3fn9738PBAXM0qVLa9zG6j6XdItyR/MwgovMnQkO4kcCq4FetcjvaWAUMDV8/mNc+jgzewI4GSiJq2YSaRYmDenJ5CeXV6pCym2Zw6QhPdOWR79+/cjPz+fxxx9n1qxZXHPNNezYsYMePXrwyCOPACRNj134zM3N5fXXXyc3N7XC6vjjj+fuu+/mS1/6Env27KFly5ZMnz6dU045hX79+nHsscfSrVs3TjvttJTWO2fOHB599FFatmzJ5z73OW699VYOOeQQLrvsMk444QQOP/zwiiorgIkTJzJ8+HBmzJjB+eefX5H+ne98h69+9av89re/5dxzz0367/vKK69k48aN9O/fH3enQ4cOzJs3j4suuoiXXnqJ448/niOOOIJTTz014fKPPfYYY8eO5e6776asrIwRI0Zwwgkn1LidyT6XdLPgmkM1bzBbCpwN/NXd+5nZWcA33P2KGpZ7HBgEHAZ8AEwB5gFzgCOAd4Dh7v5fC267+yVBa6UdwOXuXuPoOQUFBa5BdqQhW716Nccdd1zk989bXMy0F9awaWspndvnMmlIzzpdT5DAHXfcQZs2bZg4cWK2Q6l3ib6DZrbI3QsSvT9KNxdl7r7FzFqYWQt3f9nMflbTQu4+MsmswQne68C1EWIRadIu7NdFhYBkVZRCYauZtQFeAR4zsw8J7moWEWkU7rjjjmyH0GhEudD8FYIqnQnAn4G3gKGZDEpERLIjypnC1cBsdy8GZmU4HhERyaIoZwptgb+Y2atmNs7MOta4hIiINEo1Fgrufqe79yK4ENwJ+JuZ/TXjkYmISL1LpevsDwm6ptgCHJ6ZcEQk3cyMb3zjGxXT5eXldOjQgQsuuKBW63v66aeZOnVqusKTBibKzWvfAYYDHYDfA99291WZDkxE0qN169asWLGC0tJScnNzefHFF+nSpfbNXocNGxapL6FsKi8vZ7/99ks6nczu3bsr9T3UHEU5U+gG3ODuvdz9DhUIIhm0bA7c3xvuaB88L5uTltWed955PPfccwA8/vjjjBy59zaif//735x66qn069ePL3zhC6xZswYI+gIaM2YMAMuXL6d3797s2LGDmTNnMm7cOCC4y3ns2LGccsop9OjRgwULFjBmzBiOO+44Ro8eXZFHmzZtKl7PnTu3Yl7U5eMtWrSIgQMHMmDAAIYMGcJ77wWdH1TtQrrq9Pz58+nXrx99+vRhzJgxFR3R5eXlcfPNN9O/f/+K7ieasyjXFCa7+5J6iEWkeYuNIV7yLuB7xxBPQ8EwYsQInnjiCXbu3MmyZcs4+eSTK+Yde+yxvPrqqyxevJi77rqLW2+9FYDrr7+e9evX89RTT3H55Zfz0EMPVfQ6Gu/jjz/m9ddf5/7772fYsGFMmDCBlStXsnz58qQdv9V2+bKyMq677jrmzp3LokWLGDNmDN/73vcq5u/atYvCwkJuvPHGStPXXnsto0ePZvbs2Sxfvpzy8nIefHDvsC2HHnoob7zxBiNGjEhltzZJUZqkikh9yOAY4vn5+WzcuJHHH3+c8847r9K8kpISRo0axbp16zCzio7eWrRowcyZM8nPz+fqq69O2ifR0KFDMTP69OlDx44d6dOnDwC9evVi48aN9O3bt9rYUll+zZo1rFixgi9+8YtAUN0T66Ib9u1COja9Zs0aunfvzjHHHAMEXU9Pnz6dG264IeFyzZkKBZGGIsNjiA8bNoyJEyeyYMECtmzZUpF+2223cdZZZ/HUU0+xceNGBg0aVDFv3bp1tGnTptphOGvqMhuCi90x8d1VR10+xt3p1asXr7/+esJYGkLX041dlDGav5wg7ZrMhCPSjGV4DPExY8YwZcqUin/iMSUlJRUXnmfOnFkpffz48bzyyits2bKlToPQd+zYkdWrV7Nnzx6eeqrGId6T6tmzJ5s3b64oFMrKyli5cmWk5TZu3Mj69euBzHY93dhFudB8m5lVDCFkZjcRdH0hIumU4THEu3btyvjx4/dJv+mmm5g8eTL9+vWr9M98woQJXHvttRxzzDH8+te/5pZbbkk67nBNpk6dygUXXMAXvvCFStU9qdp///2ZO3cuN998MyeccAJ9+/atGEu5Oq1ateKRRx7h0ksvpU+fPrRo0YJrrtF/20SidJ19GPAsMImga+tjCcZT3pX58KqnrrOloUu162yWzQmuIZQUBWcIg2/X8LFSJ2nvOtvdPwoH2vkrsAi4xGsqSUSkdjSGuGRZ0kLBzLYBDlj4vD/QA7jEzNzdD6qfEEVEpL4kLRTcvW19BiLSVLl7pdY3IvWlNpU6UVofXWRm7eKm25vZhSnnJNIMtWrVii1bttTqxylSF+7Oli1baNWqVUrLRblPYYq7V7Qhc/etZhYbb1lEqtG1a1eKiorYvHlztkORZqhVq1Z07Zpak+YohUKiswnd9CYSQcuWLenevXu2wxCJLMp9CoVmdp+ZHRU+7iNohSQiIk1MlELhOmAXMDt8fEYw4I6IiDQxUe5T2A7cYmZtg0n/NPNhiYhINkRpfdTHzBYDK4CVZrbIzHpnPjQREalvUaqPHgK+6+5HuvuRwI3AjMyGJSIi2RClUGjt7i/HJtx9AaB+ZkVEmqAoTUs3mNltwKPh9DeADZkLSUREsiXKmcIYoAPwJPAH4DDg8kwGJSIi2RHlTOEcd6/UCbuZXQpohGsRkSYmypnC5IhpIiLSyFXXdfaXgfOALmb2QNysg4DyxEtFY2YTgCsJuuReTlAd1Ql4AjiU4I7pbzaEgXxERJqT6s4UNgGFwE6Cg3Ts8TQwpLYZmlkXYDxQ4O69gRxgBPBj4H53/zzwMXBFbfMQEZHaqW48haXAUjP7P3cvAzCzg4Fu7v5xGvLNNbMy4EDgPeBs4Gvh/FnAHcCDdcxHRERSEOWawotmdpCZHQK8Afyvmd1f2wzdvRi4F/gPQWFQQnAGstXdY9VSRUCX2uYhIiK1E6VQaOfunwAXA79195OBwbXNMDzb+ArQHehMcCPcuSksf5WZFZpZofqoFxFJryiFwn5m1gkYDjybhjzPAd52981htdSTwGlAezOLVWd1BYoTLezuM9y9wN0LOnTokIZwREQkJkqhcBfwArDe3ReaWQ9gXR3y/A9wipkdaMHAtYOBVcDLwCXhe0YBf6xDHiIiUguWjbFjzexO4DKCpq2LCZqndiFoknpImPYNd/+suvUUFBR4YWFhhqMVEWlazGyRuxckmpeVYTXdfQowpUryBuCkLIQjIiKhKNVHIiLSTKhQEBGRCtV1c/Hd6hZ09/vSH46IiGRTddcU2tZbFCIi0iBU183FnfUZiIiIZF+NrY/MrBVB53S9gFaxdHcfk8G4REQkC6JcaH4U+BxBz6h/I7jbeFsmgxIRkeyIUih83t1vA7a7+yzgfODkzIYlIiLZEKVQKAuft5pZb6AdcHjmQhIRkWyJckfzjLBn09sIBthpA9ye0ahERCQraiwU3P3h8OXfgB6ZDUdERLIpSuujhGcF7n5X+sMREZFsilJ9tD3udSvgAmB1ZsIREZFsilJ99NP4aTO7l2B8BRERaWJq0yHegQT3KoiISBMT5ZrCciA2Ek8O0AH4QSaDEhGR7IhyTeGCuNflwAfuXp6heEREJIuiVB/d7e7vhI9idy83s0czHpmIiNS7KIVCr/gJM9sPGJCZcEREJJuSFgpmNtnMtgH5ZvZJ+NgGfAD8sd4iFBGRepO0UHD3e9y9LTDN3Q8KH23d/VB3n1yPMYqISD2JUn30bzNrF5sws/ZmdmHmQhIRkWyJUihMcfeS2IS7bwWmZCwiERHJmiiFQqL3RGnKKiIijUyUQqHQzO4zs6PCx33AokwHJiIi9S9KoXAdsAuYHT4+A67NZFAiIpIdUTrE2w7cUg+xiIhIlkXp+6gDcBPBTWytYunufnYG4xIRkSyIUn30GPAm0B24E9gILMxgTCIikiVRCoVD3f3XQJm7/83dxwA6SxARaYKiFApl4fN7Zna+mfUDDqlLpuENcHPN7E0zW21mp5rZIWb2opmtC58PrkseIiKSuki9pIZ3NN8ITAQeBibUMd+fA39292OBEwiG97wFmO/uRwPz0cVtEZF6F6X10bPhyxLgrLpmGBYwZwKjw/XvAnaZ2VeAQeHbZgELgJvrmp+IiERXm+E466o7sBl4xMwWm9nDZtYa6Oju74XveR/omIXYRESatWwUCvsB/YEH3b0fsM99EO7u7B0CtBIzu8rMCs2scPPmzRkPVkSkOclGoVAEFLn7v8LpuQSFxAdm1gkgfP4w0cLuPsPdC9y9oEOHDvUSsIhIcxHl5rX2wLeAvPj3u/v42mTo7u+b2btm1tPd1wCDgVXhYxQwNXzWQD4iIvUsSm+nfwL+CSwH9qQp3+uAx8xsf2ADcDnBWcscM7sCeAcYnqa8REQkoiiFQit3/246M3X3JUBBglmD05mPiIikJso1hUfN7Ntm1im8wewQM6vTzWsiItIwRTlT2AVMA77H3hZBDvTIVFAiIpIdUQqFG4HPu/tHmQ5GRESyK0r10XpgR6YDEWkSls2B+3vDHe2D52Vzsh2RSEqinClsB5aY2csEo64BtW+SKtJkLZsDz4yHstJguuTdYBogX43ppHGIUijMCx8iUp35d+0tEGLKSoN0FQrSSETpEG9WeD/BMWHSGncvq24ZkWappCi1dJEGqMZrCmY2CFgHTAd+Baw1szMzG5ZII9Sua2rpIg1QlAvNPwW+5O4D3f1MYAhwf2bDEmmEBt8OLXMrp7XMDdJFGokohULLsI8iANx9LdAycyGJNFL5w2HoA9CuG2DB89AHdD1BGpUoF5oXmdnDwO/C6a8DhZkLSaQRyx+uQkAatSiFwjXAtUCsCeqrBNcWRESkiam2UDCzHGBpOJbyffUTkoiIZEu11xTcfTewxsyOqKd4REQki6JUHx0MrDSzfxPc3QyAuw/LWFQiIpIVSQsFMzvA3T8DbqvHeEREJIuqO1N4nWDs5Cvd/Zv1FI+IiGRRdYXC/mb2NeALZnZx1Znu/mTmwhIRkWyorlC4huCehPbA0CrzHFChICLSxCQtFNz978DfzazQ3X9djzGJiEiW1NjNhQoEEZHmI0rfRyIi0kyoUBARkQpRbl4jbH10OsEF5r+7+1MZjUpERLIiyiA7vyJoibQcWAFcbWbTMx2YiIjUvyhnCmcDx7m7A5jZLGBlRqMSEZGsiHJNYT0Q3yFetzBNRESamChnCm2B1WGHeA6cBBSa2dOgjvFERJqSKIWCBpgVEWkmaiwU3P1v9RGIiIhkX5TWR6eY2UIz+9TMdpnZbjP7pK4Zm1mOmS02s2fD6e5m9i8zW29ms81s/7rmISIiqYlyofmXwEhgHZALXAmko0nq9cDquOkfA/e7++eBj4Er0pCHiIikINIdze6+Hshx993u/ghwbl0yNbOuwPnAw+G0ETR9nRu+ZRZwYV3yEBGR1EW50LwjrMpZYmY/Ad6j7t1j/Ay4iaBlE8ChwFZ3Lw+ni4AudcxDRERSFOXg/k0gBxhHMEZzN+Crtc3QzC4APnT3RbVc/iozKzSzws2bN9c2DBERSSBK66N3wpelwJ1pyPM0YJiZnQe0Ag4Cfg60N7P9wrOFrkBxknhmADMACgoKPA3xiIhIKErro+VmtqzK41Uzu9/MDk01Q3ef7O5d3T0PGAG85O5fB14GLgnfNgr4Y6rrFhGRuolyTeF5YDfwf+H0COBA4H1gJvsO1VlbNwNPmNndwGJAg/uIiNSzKIXCOe7eP256uZm94e79zewbdcnc3RcAC8LXGwi60BARkSyJcqE5x8wqDtZmdiLBhWeA8sSLiIhIYxTlTOFK4Ddm1iac3gZcYWatgXsyFpmIiNS7KK2PFgJ9zKxdOF0SN3tOpgITEZH6F2k4TtinMBARkSaorncmi4hIE6JCQUREKqRUKJjZjEwFIiIi2ZfqmUJBRqIQEZEGIdVC4cOMRCEiIg1CSoWCu9dpHAUREWnYdKFZREQqRL5PoamYt7iYaS+sYdPWUjq3z2XSkJ5c2E/j+YiIQA1nCmaWY2YT6iuYTJu3uJjJTy6neGspDhRvLWXyk8uZtzjh0A0iIs1OtYWCu+8GRtZTLBk37YU1lJbtrpRWWrabaS+syVJEIiINS5Tqo3+Y2S+B2QTDcQLg7m9kLKoM2bS1NKV0EZEGZ9kcmH8XlBRBu64w+HbIH5621UcpFPqGz3fFpTlwdtqiqCed2+dSnKAA6Nw+NwvRiIikaNkceGY8lIXHsZJ3g2lIW8FQY+sjdz8rwaPRFQgAk4b0JLdlTqW03JY5TBrSM0sRiYikYP5dewuEmLLSID1NajxTCLvMngKcGSb9DbirMfaaGmtlpNZHItIolRSlll4LUaqPfgOsAGLnJt8EHgEuTlsU9ejCfl1UCIhI49Sua1BllCg9TaLcvHaUu09x9w3h406gR9oiEBGRaAbfDi2rXANtmRukp0mUQqHUzE6PTZjZaYCa64iI1Lf84TD0AWjXDbDgeegD9d766Brgt7HhOIGPgVFpi0BERKLLH57WQqCqKIXCJ+5+gpkdBODun5hZ94xFJCIiWROl+ugPEBQG7v5JmDY3cyGJiEi2JD1TMLNjgV5AOzOLb2l0ENAq04GJiEj9q676qCdwAdAeGBqXvg34dgZjEhGRLElaKLj7H4E/mtmp7v56PcYkIiJZEuVC82Izu5agKqmi2sjdx2QsKhERyYooF5ofBT4HDCHo4qIrQRWSiIg0MVEKhc+7+23AdnefBZwPnJzZsEREJBuiFApl4fNWM+sNtAMOr22GZtbNzF42s1VmttLMrg/TDzGzF81sXfh8cG3zEBGR2olSKMwID9DfB54GVgE/qUOe5cCN7n48cApwrZkdD9wCzHf3o4H54bSIiNSjGi80u/vD4ctXSENHeO7+HvBe+Hqbma0GugBfAQaFb5sFLABurmt+IiISXY1nCmb2IzNrHzd9sJndnY7MzSwP6Af8C+gYFhgA7wMd05GHiIhEF6X66MvuvjU24e4fA+fVNWMza0PQhcYNcd1nxPJwgiE/Ey13lZkVmlnh5s2b6xqGiIjEiVIo5JjZAbEJM8sFDqjm/TUys5YEBcJj7v5kmPyBmXUK53cCPky0rLvPcPcCdy/o0KFDXcIQEZEqohQKjwHzzewKM7sCeJGgzr9WzMyAXwOr3f2+uFlPs7dL7lHAH2ubh4iI1E6UC80/NrNlwOAw6Qfu/kId8jyNYEjP5Wa2JEy7FZgKzAkLnnfYO/yniEjdLJsTDG5fUhQMXTn49oyOSdCYRenmAnd/Hng+HRm6+98BSzJ7cJJ0EZHaWTYHnhkPZeGAkSXvBtOggiGBpNVHZvb38HmbmX0S99hmZp8kW05EpEGZf9feAiGmrDRIl31U10vq6eFz2/oLR0QkzUqKUktv5qobZOeQ6hZ09/+mPxyRxm3e4mKmvbCGTVtL6dw+l0lDenJhvy7ZDqt5a9c1qDJKlC77qK710SKgMHzeDKwF1oWvF2U+NJHGZd7iYiY/uZziraU4ULy1lMlPLmfe4uJsh9a8Db4dWuZWTmuZG6TLPpIWCu7e3d17AH8Fhrr7Ye5+KMFobH+prwBFGotpL6yhtGx3pbTSst1Me2FNliISILiYPPQBaNcNsOB56AO6yJxElNZHp7h7xfCb7v68mdWlQzyRJmnT1tKU0qUe5Q9XIRBRlJvXNpnZ980sL3x8D9iU6cBEGpvO7XNTShdpiKIUCiOBDsBT4ePwME1E4kwa0pPcljmV0nJb5jBpSM8sRSSSuih3NP8XuL4eYhFp1GKtjNT6SBqzGgsFMzsGmAjkxb/f3c/OXFgijdOF/bqoEJBGLcqF5t8D/wM8DOyu4b0itaO+aUQahCiFQrm7P5jxSKT5Ut80Ig1GlAvNz5jZd8ysk5kdEntkPDJpPtQ3jUiDEeVMITbGwaS4NCcN4zWLAOqbRqQBidL6qHt9BCLNmPqmEWkwqusQ72x3f8nMLk40P24YTZG6GXx75WsKoL5pRLKkujOFgcBLwNAE8xxQoSDpEbuYrNZHDYtahDVIme6J19w9bSurbwUFBV5YWJjtMESanqotwiA4e1NHclkV64k3vuPF3JY53HNxn5QKBjNb5O4FieZFaX0kIs2NWoQ1SPXRE68KBRHZl1qENUj10RNvtYWCmbUwsy+kLTcRaRyStfxSi7Csqo+eeKstFNx9DzA9bbmJSOOg0coapProiTdK9dF8M/uqmVnachWRhk2jlTVIF/brwj0X96FL+1wM6NI+N+WLzDWpsfWRmW0DWhN0hlcKGODuflDaoqilZt/6SE0GRaQWqmt9FOWO5rbpD0nqTJ3IiUSW6bb9TUmUvo8ws2HAmeHkAnd/NnMhSSTVNRlUoSBSoWrb/uKtpUx+cjmACoYEarymYGZTCUZeWxU+rjezezIdmNSgiTUZnLe4mNOmvkT3W57jtKkvMW9xcbZDkiaiPtr2NyVRzhTOA/qGLZEws1nAYmByJgOTGjShTuT0T65haipVLvXRtr8piXrzWvu41+0yEIekqgk1GdQ/uYYnVlAXby3F2VtQN8YzuPpo29+URCkUfgQsNrOZ4VnCIuCHmQ1LapQ/nIV97uR9OrDHjffpwMI+dzbK6wn6J9fwNKWCuj7a9jcl1VYfmVkLYA9wCnBimHyzu7+fiWDM7Fzg50AO8LC7T81EPk3BvMXFTF54JKVlP69Iy12Ywz3dihvdKX7n9rkUJygA9E8ue5pSQR37PTSFqrD6UG2h4O57zOwmd58DPJ3JQMwsh+Du6S8CRcBCM3va3VdlMt/Gqrp/co3tyz5pSM+EPT/qn1z2NLWC+sJ+XRrd7yJbolQf/dXMJppZtwyP0XwSsN7dN7j7LuAJ4CsZyKdJaGr/5DJ9l6akRlUuzVeU1keXhc/XxqVlYozmLkB8c5oi4OQ059Fk6J+cZJKqXJqvKNcUbnH32fUUT43M7CrgKoAjjjgiy9Fkj6pcJNNUUDdPUXpJnVRPsRQD3eKmu4ZpVWOa4e4F7l7QoUOHegqt4VGVi4hkQpTqo7+a2URgNrA9luju/01zLAuBo82sO0FhMAL4WprzaFL0T05E0q3BXFNw93IzGwe8QNAk9TfuvjKdeYiISPWi9JLavT4CCfP6E/Cn+spPREQqS3pNwcxuint9aZV5P8pkUCIikh3VXWgeEfe6aud352YgFhERybLqCgVL8jrRtIiINAHVXVPwJK8TTWfFokWLPjKzd2q5+GHAR+mMJ4u0LQ1PU9kO0LY0VHXZliOTzUg6RrOZ7SZogmpALrAjNgto5e4taxlMg2BmhcnGKG1stC0NT1PZDtC2NFSZ2pakZwrunpNsnoiINE1RB9kREZFmoDkXCjOyHUAaaVsanqayHaBtaagysi1JrymIiEjz05zPFEREpCp3bxQP4EKCprDHhtN54fTdce85DCgDflll2SXAE3HTOWFa/OMjYHY4f3/gZ8B6YB3wR6Br3PIO/DRueiJwRwa3szSMcRXwPwSFeXXpKxKsdybwdtz2vpZiXLur7K+8MP0GYCfQrsr7vwwUhrEtju0v4A6CDg/j19UeGASUhNPLgL8Ch8et7yrgzfDxb+D0uHk1fV6fxr0+D1gLHJlgGzsC/wdsIBiL/HXgonDe6WG+sRiuqrJsdfHtRzDW+bq4bf5eovji0hLup7jv3++qrH8z8Gw4PZq43wDwLWAFsDz8LCYm+E4sBQansE+/B6wMP6slwMkRl6v6Pbo87vWuMMYlwNQk38ND497/fpV95FXWfUu4zAXhdi8l+D5eHbePJybJJ3YsuSbDx7WLqsS8hGAI5C8DvYCXgDXhvryNsHYn7lixDFgd7rcLq/zedwBt49J+Fu6jw6qNKZMbnOadNxt4FbgznM4j+PEujnvP2HCnxv8gjgt3WDHQOsm6OxEM8NM7nL4X+DWQE05fTvBDj1W37ST4MR0WTqezUEi0nSvC1/sBrwAXR0mvst6ZwCV1iGufA1eY/q8w3svj0noDb7G3YMsBxoavE/4QCQqFZ+Om74nbBxcQHKRj+7s/8B/gcxE/r0/D58EEB6ujEuRvBIXANXFpRwLXAZ8L8+sfph8WxnN+xPimhvu/VTjdNv77kmjfJttPsfcTfM9zw+kvh9P7FArhvDeAzuH0AcC3q34ngLOAdXF5JN2nwKnhvjogbn90rmm56r5H4byN1HDAqm4fJdmPLYFNhAVTuP09I+zjsQTf67+l43edwjZdBfwNaE3wG/pSmH4g8DxwbTh9Qvhd7h5Odw+n8+M+22XAN8LpFuF0UU37uFFUH5lZG4J/aldQufuNHcBqM4u11b0MmFNl8ZHAo8BfSDC8p5kZMAuY5u4rzOxAgi/yBHffDeDujwCfAWeHi5UTXOSZUPetqxRLsu0kjKMceA34fJT0TDOzo4A2wPcJ9nPMTcAP3f3NML7d7v5gCus1ggPnx2HSzcAkd/8oXN8bBJ/ZtRE/L8zsTOB/gQvc/a0E2Z4N7HL3/4kluPs77v4Lgh6CZ4b5EsZxE3BLxPi+DVzn7jvD+dvc/Y6o+yOJPwHnh69HAo8ned9kggPfpjDvz9z9fxO873WC0Q+JsE87AR+5+2fhvI/cfVPUz6KetSX407QljOczd18TYbmRwI1AFzPrmsH4KpjZMcDtwDcJfv//cPe/ALj7DmAce79zE4Efufvb4fy3Cf5IxY9/8wR7e7keBPyD4NhVrUZRKBAczP/s7muBLWY2IG7eE8AIM+tGcGq6qcqyl4XveZzKB66YCQQ76hfh9OeB/7j7J1XeV0hwOhczHfi6mbWrxfYkU912xn6sgwnOfGpMT2CamS0JH4+lGFtu3LJPhWkjCPbtq0BPM+sYpvcm+OeczIS4db0cl36GmS0h+Jd9DvCbML1XgvXFPo8on9cBwDyC0+s3k8TUi+AfdbJ5yfKPGt+2JOuuTrL9BHu/962AfIIztkRq+ixiziXYR1DzPv0L0M3M1prZr8xsYMTlIPH3KF3i173EzC7zYNyXp4F3zOxxM/t6OKJkUuGxpJO7/5vgT+Zl1b0/HcysJUHV5Y3u/h8SfKfCPzNtzOygRPPZ9xi1FuhgZgcTHPueiBJLYykU4jfoCSof3P8MfJHgAFVp2NDwDOKjcCfPB/qZ2SFx808gqBO/3MNzrKjCL/5vgfEpbUn1km3nUeHB8h/Ac+7+fA3pyUxy977h4+spxlYat+xF8fF6MELfH4BLky9eyf1x6zorLv3VMK0b8AjwkxRjTKaM4EzqiqgLmNl0M1tqZgvTFENsvZeHB6x3w4NPdZLtJ9x9GUFV4Ujq1t38NDNbS3BA+nGUBdz9U2AAQVXHZmC2mY2OmF+i71G6xK+7r4fDCLv7lQR/mv5N8A/7N9WthMo1DlWPN5nyA2Clp3/o4ycJjo0nE/x5q1GDLxTCg/jZwMNmtpHg9Gg4Yad87r6LoMS8EZhbZfGRwLHhcm8BBwFfDdebCzxGUNf9QdwybwFHmFnbKusaQHBhLd7PCA40rWu9gaEatvOt8Ever0q1Q7L0jDOzPsDRwIthvCPY++NZSbC/6uJp4Mzw9aoE64t9HlE+rz0E+/IkM7s1SX4rCa4FAODu1xIcSDrUkH9N8a2Pj8/dH3H3vgQX1evaa8DTBHX4yaqOoObPYpK7H0NQBRY7WNa4T8MqwQXuPoWgWuOrUZbLFndf7u73E/yB/GoNbx8JjA6/108D+WZ2dKZiM7NBYUzj4pL3+U6ZWQ+C6yafJJpP4v08m6DAeTH881ajBl8oAJcAj7r7ke6eF/6LfJvK4zn/FLjZ44YIDU8RhwN9wuXyCKpnYgeuewkuIj0Xn5m7byeoD77PzHLCdX2L4ELPS1Xe+1+CfxSR/4HWcTsbkpEEF0vzwkdnoLOZHQlMA24N60gxsxZmdk2K6z+d4CADwRnDj83s0HB9fQkupv4q6ucV1smeT1Dll+jzegloZWZj49IODJ+nExwk+obrP5TgX3XsTKa6+HYQXHj9ZVjVQxjn/inuj0R+Q3Axvrpqw3sIzgY+F+a9v5ldmeB9vwRamNmQmvapmfWscpDsC7yTym+nvphZm/CgG9MXSNqJZvidbePuXeKOG/eQobOFsGrnEeBbVaoYHwNON7NzwvflAg+w9zt3LzDZzPLC+XnArQTHwgru/g5BS7FfRQ7K6/HKem0ewMvAuVXSxhNciU/UymY0wRd8IPDPKvNyCJqxdSFomrWayk3BHvO9LRR+QXBQWgc8A3SLW098E8eOBBe876jn7cyrJr2MoJVB7HEp+zZJXQLsn0J8n1aZ3kDYuigu7T6Cwhn2tshZTfCv5idh+h3s29Qyj8pNUpcStKY6Jm7dYwma5r1JMJ73mXHzUvm8YoXtsATb2ImguuBtgqqGl4HLwnlnhvm+GcYxtsqy1cXXkqAF0nqCppGvEfxQ9w/n76nyeX032X5K9FmEaYNI3iT1coImqSvD5++G6TOJa5FG8G91fk37lOAf6Wvh57qMoIrisFQ/iwTbsJG6tT6q2tx1KsGF5j+Fn80SgqrWgrjlt1bZ91Oo0hyW4JrN6rr8vqvZhskEHY8uqfK4DOgDLAhjXx/GFt8k9WKC64hvhs8Xx82r9Nmmso91R7OIiFRoDNVHIiJST1QoiIhIBRUKIiJSQYWCiIhUUKEgIiIVVCiIiEgFFQoiIlJBhYKIiFT4f4uLYoAANZ4DAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "prep_csv_dict = cast_string_to_double(csv_dict)\n",
        "pred_dict = perform_regression_dict(prep_csv_dict)  \n",
        "rmse_dict, max_abse_dict = compute_rmse_dict(pred_dict)\n",
        "\n",
        "plt.scatter(x=rmse_dict.keys(), y=rmse_dict.values(), label=\"Root mean squared error\")\n",
        "plt.scatter(x=max_abse_dict.keys(), y=max_abse_dict.values(), label=\"Maximum error\")\n",
        "plt.ylabel(\"Error indicator - gap from actual stock value\")\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BIGDATA_newmagicwand.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
